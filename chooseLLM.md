# üß† RESUMEN: Fundamentos de Modelos de Lenguaje y C√≥mo Elegir uno

## 1. ¬øQu√© es un modelo de lenguaje (LLM)?

Un modelo de lenguaje (Large Language Model) es una red neuronal entrenada para predecir la siguiente palabra en una secuencia de texto. Aprende a partir de grandes corpus (como libros, webs, c√≥digo, etc.) y luego puede generar texto, responder preguntas, traducir, etc.

## 2. ¬øQu√© define la potencia de un modelo?
| Par√°metro                                    | Descripci√≥n                                                                                                                                                                |
| -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Tama√±o del modelo (n√∫mero de par√°metros)** | Cuantos m√°s par√°metros tiene una red, m√°s complejidad puede aprender. Ej: GPT-2 ‚Üí 124M, Mistral ‚Üí 7B, GPT-3 ‚Üí 175B                                                         |
| **Token context size (n\_ctx)**              | Cu√°ntos tokens (palabras divididas) puede recordar en una petici√≥n. Mayor tama√±o = m√°s contexto, pero m√°s RAM.                                                             |
| **Cuantizaci√≥n (Q4, Q5, etc.)**              | T√©cnica para reducir el tama√±o del modelo (menos bits por par√°metro). Ahorra RAM, pero puede perder algo de precisi√≥n.                                                     |
| **Modelo base vs. instruccional**            | Modelos como `mistral-7b-instruct` est√°n ajustados para seguir instrucciones humanas. Son mejores para chat.                                                               |
| **Embedding model**                          | Separa de los LLM. Este modelo convierte texto en vectores. Usado para comparar similitud sem√°ntica en b√∫squedas. Ej: `all-MiniLM-L6-v2`. Muy ligeros comparados con LLMs. |


## 3. ¬øQu√© influye en el rendimiento?
- RAM: Fundamental. Cargar un modelo de 7B aunque sea cuantizado (Q4) puede requerir 6‚Äì8 GB reales.

- CPU vs. GPU: CPU puede ejecutar modelos peque√±os (hasta ~3B) pero es muy lenta comparado con GPU.

- Threads y n√∫cleos: M√°s hilos = mejor inferencia en CPU, hasta cierto punto.

## 4. ¬øC√≥mo saber qu√© modelo usar?
| Uso                                     | Recomendaci√≥n general                               |
| --------------------------------------- | --------------------------------------------------- |
| Solo responder preguntas sencillas      | Modelos <1B (e.g., TinyLlama, Flan-T5-small)        |
| Procesamiento de documentos + preguntas | Embeddings + modelos peque√±os tipo instruct         |
| Uso educativo o pruebas locales         | Modelos cuantizados Q4 (ej. `mistral-7b.Q4`) o Tiny |
| Producci√≥n en nube                      | GPT-3.5/4 v√≠a API o modelos grandes en GPU          |

## 5. Diferencias clave entre modelos populares
| Modelo              | Tama√±o    | Tipo       | Uso t√≠pico        | RAM m√≠nima (Q4 en CPU)   |
| ------------------- | --------- | ---------- | ----------------- | ------------------------ |
| **GPT-2**           | 124M‚Äì1.5B | Generativo | B√°sico            | \~2‚Äì4 GB                 |
| **Flan-T5-small**   | 80M       | Instruct   | Preguntas simples | \~1‚Äì2 GB                 |
| **TinyLlama-1.1B**  | 1.1B      | Generativo | Pruebas           | \~3 GB                   |
| **Mistral-7B**      | 7B        | Instruct   | Chat avanzado     | \~6‚Äì8 GB (justo para ti) |
| **GPT-3.5 / GPT-4** | 175B+     | Instruct   | Nube/API          | N/D                      |

## ‚úÖ Recomendaci√≥n para tu caso (4 CPUs, 6GB RAM)
Mistral-7B est√° justo en el l√≠mite y por eso te tarda tanto (y se cuelga). Lo mejor:
üîÅ Sustituir por un modelo m√°s liviano

1. Flan-T5-small o Flan-T5-base (de HuggingFace Hub):
- Instruccional, responde bien a preguntas tipo QA.
- Mucho m√°s r√°pido que Mistral.
- Puedes usarlo v√≠a HuggingFaceHub si tienes clave, o directamente carg√°ndolo en local.

2. TinyLlama-1.1B-chat:

- Es generativo como Mistral, pero pesa la sexta parte.

- Optimizado para CPU.

## üëâ Te recomiendo usar google/flan-t5-small ahora mismo.
Va perfecto para preguntas b√°sicas y se carga r√°pido. Si luego quieres mejorar, puedes probar con flan-t5-base o TinyLlama.